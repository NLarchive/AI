{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLarchive/AI/blob/main/notebookchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epe47xOmKVSv"
      },
      "source": [
        "# AUTODEBUG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: API Setup and Authentication ---\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "gemini_api_secret_name = 'GOOGLE_API_KEY'\n",
        "\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get(gemini_api_secret_name)\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except userdata.SecretNotFoundError as e:\n",
        "    print(f'''Secret not found\\n\\nThis expects you to create a secret named {gemini_api_secret_name} in Colab\\n\\nVisit https://makersuite.google.com/app/apikey to create an API key\\n\\nStore that in the secrets section on the left side of the notebook (key icon)\\n\\nName the secret {gemini_api_secret_name}''')\n",
        "    raise e\n",
        "except userdata.NotebookAccessError as e:\n",
        "    print(\n",
        "        f'''You need to grant this notebook access to the {gemini_api_secret_name} secret in order for the notebook to access Gemini on your behalf.''')\n",
        "    raise e\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"There was an unknown error. Ensure you have a secret {gemini_api_secret_name} stored in Colab and it's a valid key from https://makersuite.google.com/app/apikey\")\n",
        "    raise e\n",
        "\n",
        "llm = genai.GenerativeModel('models/gemini-1.5-flash-001')\n",
        "\n",
        "# --- Cell 2: Context Caching Setup ---\n",
        "def setup_cache(system_instruction, contents, ttl_minutes):\n",
        "    if len(contents) < 32768:  # Ensure the minimum token requirement for caching\n",
        "        print(\"Caching skipped: content does not meet the minimum token requirement.\")\n",
        "        return None\n",
        "    cache = caching.CachedContent.create(\n",
        "        model='models/gemini-1.5-flash-001',\n",
        "        display_name='notebook_context_cache',\n",
        "        system_instruction=system_instruction,\n",
        "        contents=contents,\n",
        "        ttl=datetime.timedelta(minutes=ttl_minutes),\n",
        "    )\n",
        "    return cache\n",
        "\n",
        "# --- Cell 3: LLM Interaction Class ---\n",
        "class LLM:\n",
        "    def __init__(self, cache=None):\n",
        "        self.model = genai.GenerativeModel.from_cached_content(cache) if cache else genai.GenerativeModel('models/gemini-1.5-flash-001')\n",
        "        self.chat = None\n",
        "\n",
        "    def chat_with_llm(self, message: str) -> str:\n",
        "        if self.chat is None:\n",
        "            self.chat = self.model.start_chat(history=[])\n",
        "\n",
        "        response = self.chat.send_message(message)\n",
        "        return response.text\n",
        "\n",
        "# --- Cell 4: Notebook Debugger Class ---\n",
        "import nbformat\n",
        "\n",
        "class NotebookDebugger:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def analyze_code(self, code):\n",
        "        prompt = f\"You are an expert Python debugger. Analyze the following code for potential issues, inefficiencies, or improvements:\\n\\n{code}\"\n",
        "        analysis = self.llm.chat_with_llm(prompt)\n",
        "        return analysis\n",
        "\n",
        "    def suggest_improvements(self, code):\n",
        "        prompt = f\"You are an expert Python developer. Suggest improvements for the following code, focusing on efficiency, readability, and best practices:\\n\\n{code}\"\n",
        "        suggestions = self.llm.chat_with_llm(prompt)\n",
        "        return suggestions\n",
        "\n",
        "# --- Cell 5: Notebook Chat Function ---\n",
        "def chat_with_notebook(file_path: str, cache=None):\n",
        "    llm_instance = LLM(cache)\n",
        "    debugger = NotebookDebugger(llm_instance)\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            notebook = nbformat.read(f, as_version=4)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {file_path} was not found.\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: The file {file_path} is not a valid Jupyter notebook.\")\n",
        "        return\n",
        "\n",
        "    print(\"Notebook loaded. You can now debug and improve the code in this notebook.\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nOptions:\")\n",
        "        print(\"1. Analyze a specific cell\")\n",
        "        print(\"2. Suggest improvements for a specific cell\")\n",
        "        print(\"3. Analyze entire notebook\")\n",
        "        print(\"4. Chat with LLM about the notebook\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-5): \")\n",
        "\n",
        "        if choice in ('1', '2'):\n",
        "            try:\n",
        "                cell_num = int(input(\"Enter the cell number to analyze/improve: \")) - 1\n",
        "                if 0 <= cell_num < len(notebook.cells):\n",
        "                    code = notebook.cells[cell_num].source\n",
        "                    if choice == '1':\n",
        "                        result = debugger.analyze_code(code)\n",
        "                        print(\"\\nAnalysis result:\")\n",
        "                    else:\n",
        "                        result = debugger.suggest_improvements(code)\n",
        "                        print(\"\\nSuggested improvements:\")\n",
        "                    print(result)\n",
        "                else:\n",
        "                    print(\"Invalid cell number.\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid cell number.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            for i, cell in enumerate(notebook.cells):\n",
        "                if cell.cell_type == 'code':\n",
        "                    print(f\"\\nAnalyzing Cell {i + 1}:\")\n",
        "                    result = debugger.analyze_code(cell.source)\n",
        "                    print(result)\n",
        "\n",
        "        elif choice == '4':\n",
        "            while True:\n",
        "                query = input(\"Enter your question about the notebook (or type 'back' to return to main menu): \")\n",
        "                if query.lower() == 'back':\n",
        "                    break\n",
        "                prompt = f\"Here is a Jupyter Notebook with various code cells:\\n\\n\"\n",
        "                for cell in notebook.cells:\n",
        "                    if cell.cell_type == 'code':\n",
        "                        prompt += f\"Cell {cell.execution_count if hasattr(cell, 'execution_count') else 'N/A'}:\\n{cell.source}\\n\\n\"\n",
        "\n",
        "                prompt += f\"\\nUser's question: {query}\\n\\nProvide insights or answers based on the notebook.\"\n",
        "\n",
        "                try:\n",
        "                    llm_response = llm_instance.chat_with_llm(prompt)\n",
        "                    print(\"\\nResponse:\")\n",
        "                    print(llm_response)\n",
        "                    print(\"-\" * 50)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during LLM interaction: {str(e)}\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# --- Cell 6: Main Execution Block ---\n",
        "from google.colab import drive\n",
        "import json\n",
        "if __name__ == \"__main__\":\n",
        "    drive.mount('/content/drive')\n",
        "    notebook_path = '/content/drive/MyDrive/AI/AI_modules/notebookchat.ipynb'  # Replace with your notebook path\n",
        "\n",
        "    # Setup cache with system instructions\n",
        "    system_instruction = (\n",
        "        \"You are an AI assistant for debugging and improving Jupyter Notebooks. Use best practices to provide insights.\"\n",
        "    )\n",
        "    cache = None\n",
        "    if len([]) >= 32768:  # Check if contents exceed token minimum for caching\n",
        "        cache = setup_cache(system_instruction, [], ttl_minutes=60)  # Cache with a 1-hour TTL\n",
        "\n",
        "    chat_with_notebook(notebook_path, cache)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DMuY_6iO8Lht",
        "outputId": "fc266a13-e262-4708-c5cc-cce7c2078b8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Notebook loaded. You can now debug and improve the code in this notebook.\n",
            "\n",
            "Options:\n",
            "1. Analyze a specific cell\n",
            "2. Suggest improvements for a specific cell\n",
            "3. Analyze entire notebook\n",
            "4. Chat with LLM about the notebook\n",
            "5. Exit\n",
            "Enter your choice (1-5): 3\n",
            "\n",
            "Analyzing Cell 2:\n",
            "## Analysis of the Python Code for Debugging and Improving Jupyter Notebooks\n",
            "\n",
            "This code provides a framework for interacting with a Jupyter Notebook through a chat interface, utilizing a large language model (LLM) for analysis and improvement suggestions. Here's a breakdown of potential issues, inefficiencies, and improvements:\n",
            "\n",
            "**1. API Setup and Authentication:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Secret Storage:** Using `userdata.get()` for API key retrieval makes the code dependent on Colab environment and requires manual secret creation. It's not portable to other environments.\n",
            "    * **Hardcoded Secret Name:**  `gemini_api_secret_name` should be a constant or passed as a parameter to avoid hardcoding.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **Environment Variables:** Use environment variables (`os.environ`) for storing secrets, which are more flexible and secure than Colab secrets.\n",
            "    * **Configuration File:** Use a configuration file (e.g., `.env`) to store API keys and other settings. This allows for easier management and portability.\n",
            "    * **Library-Specific Secret Handling:**  Consider using a dedicated library for handling secrets like `python-dotenv` or `keyring`.\n",
            "\n",
            "**2. LLM Interaction Class:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Model Initialization:** The `LLM` class initializes the model in every instance, leading to unnecessary overhead.\n",
            "    * **Chat Object:**  The `chat` object is created once and persists across multiple `chat_with_llm` calls, potentially leading to unexpected behavior or context leakage.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **Model Singleton:** Use a singleton pattern or a dedicated class for model initialization and management.\n",
            "    * **Resetting Chat Context:** Clear or reset the chat context (e.g., with `self.chat = None`) after each interaction to avoid context carryover.\n",
            "    * **Async Chat:** Explore asynchronous communication with the LLM using libraries like `aiohttp` for improved responsiveness.\n",
            "\n",
            "**3. Notebook Debugger Class:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Duplication:** The prompt for analyzing code and suggesting improvements is very similar. Consider using a single method with a parameter to determine the type of analysis.\n",
            "    * **LLM Integration:**  The code might be more efficient if the LLM interaction logic is delegated to the `LLM` class, reducing redundancy in the `NotebookDebugger` class.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **Single Analysis Method:** Create a single `analyze_code` method with a flag or parameter to control the type of analysis.\n",
            "    * **LLM Delegate:** Implement a `get_analysis` method in the `LLM` class that takes the prompt and analysis type as arguments.\n",
            "    * **Error Handling:**  Consider adding error handling (e.g., try-except blocks) to catch exceptions during LLM interactions.\n",
            "\n",
            "**4. Notebook Chat Function:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **User Experience:** The menu-driven interface is somewhat rigid and lacks features like error handling for invalid input or help options.\n",
            "    * **Code Analysis Efficiency:** Analyzing entire notebook in a single loop could be slow.\n",
            "    * **LLM Context:** The current approach of providing the entire notebook content in each prompt might overwhelm the LLM.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **User-Friendly Interface:** Consider using a library like `prompt_toolkit` for a more interactive and visually appealing interface.\n",
            "    * **Selective Analysis:** Allow the user to analyze specific cells or sections of the notebook instead of the whole notebook.\n",
            "    * **LLM Context Management:** Implement a mechanism to provide only the relevant code context to the LLM for each prompt.\n",
            "    * **Caching Results:** Cache LLM responses to avoid redundant calls for the same code.\n",
            "\n",
            "**5. Main Execution Block:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Hardcoded Path:**  The notebook path is hardcoded, making the code less flexible.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **User Input or Configuration:** Allow the user to specify the notebook path through user input or read it from a configuration file.\n",
            "\n",
            "**Additional Considerations:**\n",
            "\n",
            "* **Logging:**  Implement logging to record user interactions, errors, and LLM responses for debugging and analysis.\n",
            "* **Code Style:**  Maintain a consistent code style with proper indentation, naming conventions, and documentation for better readability and maintainability.\n",
            "* **Performance Optimization:** Profile the code for bottlenecks and optimize LLM interaction by using techniques like batching requests or caching.\n",
            "\n",
            "**Summary:**\n",
            "\n",
            "This code provides a good starting point for interacting with a Jupyter Notebook via an LLM-powered chatbot. However, it can be significantly improved in terms of API handling, code organization, user experience, and LLM interaction. By addressing the points mentioned above, you can create a more robust, efficient, and user-friendly application.\n",
            "\n",
            "\n",
            "Analyzing Cell 3:\n",
            "## Analysis of the Python Code for Debugging and Improving Jupyter Notebooks with Caching\n",
            "\n",
            "This code builds upon the previous version by introducing context caching to potentially speed up LLM interactions.  Here's an analysis of the code, including potential issues, inefficiencies, and improvements:\n",
            "\n",
            "**1. API Setup and Authentication:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Secret Storage:** Same issue as before - using `userdata.get()` for API key retrieval makes the code dependent on Colab environment and requires manual secret creation. It's not portable to other environments.\n",
            "    * **Hardcoded Secret Name:** `gemini_api_secret_name` should be a constant or passed as a parameter.\n",
            "* **Improvements:**\n",
            "    * **Environment Variables:** Use environment variables (`os.environ`) for storing secrets, which are more flexible and secure than Colab secrets.\n",
            "    * **Configuration File:**  Use a configuration file (e.g., `.env`) to store API keys and other settings.\n",
            "    * **Library-Specific Secret Handling:**  Consider using a dedicated library for handling secrets like `python-dotenv` or `keyring`.\n",
            "\n",
            "**2. Context Caching Setup:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Token Limit Check:** The token limit check for caching is done outside the `setup_cache` function. It might be better to consolidate this logic within the function.\n",
            "    * **Unnecessary Caching:**  The code attempts to set up the cache even if the `contents` list is empty. This could lead to unnecessary cache creation if the `len([]) >= 32768` condition is accidentally met.\n",
            "    * **Cache TTL:** The current TTL setup assumes a 60-minute (1-hour) TTL. This might be too long or too short depending on the use case. Consider making it configurable.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **Consolidate Token Check:** Move the token limit check into the `setup_cache` function.\n",
            "    * **Conditional Caching:** Add a conditional check for `contents` before creating the cache.\n",
            "    * **Configurable TTL:**  Pass the `ttl_minutes` as a parameter to the `setup_cache` function or read it from a configuration file.\n",
            "\n",
            "**3. LLM Interaction Class:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Model Initialization:**  The `LLM` class initializes the model in every instance, leading to unnecessary overhead.\n",
            "    * **Chat Object:** The `chat` object is created once and persists across multiple `chat_with_llm` calls, potentially leading to unexpected behavior or context leakage.\n",
            "    * **Caching Logic:**  The caching logic is not handled gracefully in the `__init__` method.  It's hard to understand how the model initialization happens with and without the cache.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **Model Singleton:** Use a singleton pattern or a dedicated class for model initialization and management.\n",
            "    * **Resetting Chat Context:** Clear or reset the chat context (e.g., with `self.chat = None`) after each interaction to avoid context carryover.\n",
            "    * **Cleaner Caching:** Refactor the caching logic in the `__init__` method to be more readable and organized. Create a dedicated method for initializing the model from cached content if it exists.\n",
            "\n",
            "**4. Notebook Debugger Class:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Duplication:** Same as before - the prompt for analyzing code and suggesting improvements is very similar. Consider using a single method with a parameter to determine the type of analysis.\n",
            "    * **LLM Integration:** The code might be more efficient if the LLM interaction logic is delegated to the `LLM` class, reducing redundancy in the `NotebookDebugger` class.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **Single Analysis Method:** Create a single `analyze_code` method with a flag or parameter to control the type of analysis.\n",
            "    * **LLM Delegate:**  Implement a `get_analysis` method in the `LLM` class that takes the prompt and analysis type as arguments.\n",
            "    * **Error Handling:** Consider adding error handling (e.g., try-except blocks) to catch exceptions during LLM interactions.\n",
            "\n",
            "**5. Notebook Chat Function:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **User Experience:** Same as before - the menu-driven interface is somewhat rigid and lacks features like error handling for invalid input or help options.\n",
            "    * **Code Analysis Efficiency:** Analyzing entire notebook in a single loop could be slow.\n",
            "    * **LLM Context:** The current approach of providing the entire notebook content in each prompt might overwhelm the LLM.\n",
            "    * **Cache Management:** The `cache` variable is passed as an argument, but there's no mechanism for refreshing or updating the cache after a certain time.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **User-Friendly Interface:** Consider using a library like `prompt_toolkit` for a more interactive and visually appealing interface.\n",
            "    * **Selective Analysis:**  Allow the user to analyze specific cells or sections of the notebook instead of the whole notebook.\n",
            "    * **LLM Context Management:**  Implement a mechanism to provide only the relevant code context to the LLM for each prompt.\n",
            "    * **Caching Results:** Cache LLM responses to avoid redundant calls for the same code.\n",
            "    * **Cache Refresh:** Add logic to periodically refresh the cache based on the TTL.\n",
            "\n",
            "**6. Main Execution Block:**\n",
            "\n",
            "* **Potential Issues:**\n",
            "    * **Hardcoded Path:** The notebook path is hardcoded, making the code less flexible.\n",
            "    * **Caching Logic:** The caching logic is not properly integrated with the main function. The cache is set up outside the `chat_with_notebook` function, and there's no clear mechanism for how the cache is used or updated.\n",
            "\n",
            "* **Improvements:**\n",
            "    * **User Input or Configuration:** Allow the user to specify the notebook path through user input or read it from a configuration file.\n",
            "    * **Consolidated Caching:**  Integrate the caching logic into the `chat_with_notebook` function by passing the cache object and handling cache creation and updating within the function.\n",
            "\n",
            "**Additional Considerations:**\n",
            "\n",
            "* **Logging:** Implement logging to record user interactions, errors, and LLM responses for debugging and analysis.\n",
            "* **Code Style:** Maintain a consistent code style with proper indentation, naming conventions, and documentation for better readability and maintainability.\n",
            "* **Performance Optimization:** Profile the code for bottlenecks and optimize LLM interaction by using techniques like batching requests or caching.\n",
            "\n",
            "**Summary:**\n",
            "\n",
            "This code demonstrates a basic implementation of context caching for improving the efficiency of LLM interactions. However, it still has several areas that could be improved, particularly in terms of code organization, user experience, and proper caching management. By addressing these points, you can create a more robust, efficient, and user-friendly notebook debugger.\n",
            "\n",
            "\n",
            "Options:\n",
            "1. Analyze a specific cell\n",
            "2. Suggest improvements for a specific cell\n",
            "3. Analyze entire notebook\n",
            "4. Chat with LLM about the notebook\n",
            "5. Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-525c537d4105>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem_instruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttl_minutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cache with a 1-hour TTL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mchat_with_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-525c537d4105>\u001b[0m in \u001b[0;36mchat_with_notebook\u001b[0;34m(file_path, cache)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-5): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOXqomQkXBWm5W9uDgzXwH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}